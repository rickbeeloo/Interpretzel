

![header](interpretzel/docs/fresti_banner.png)

## ü•® Install

```
git clone https://github.com/rickbeeloo/Interpretzel/tree/master
cd Interpretzel
pip install .
```
(soon: `pip install intepretzel`)

## Goal
Most genomic metadata remains unstandardized. For instance, selecting all food samples from a database is challenging because they are often recorded by dish name or ingredient (e.g., "Kimchi," "meat dish," "strawberry jam"). With the advancements in Large Language Models (LLMs), we aim to use simple prompts to standardize common genomic metadata fields, such as isolation source, host name, and isolation country. Although any ontology can be utilized, we demonstrated this approach using the [SPIRE micro-ontology](https://pubmed.ncbi.nlm.nih.gov/37897342/) for the metadata fields from [BV-BRC](https://www.bv-brc.org/)

##  Quickstart

### üß™ Isolation source

#### Generating class descriptions 
We observed that providing a description of a class provides better results, especially when class names themselves can be considered vague. For example "Contaminated", while referring to samples like "contaminated soil" one might also consider bacteria in blood a contamination. In these cases it can help to provide a one line class description. We can use `iso_desc` to use an LLM to help us with that. For example: 

`iso_desc -i class_data.txt -o classes.json -m "mistralai/Mistral-7B-Instruct-v0.2"`

Where:
- `i`, a two column file with the columsn `class_name` and `examples`, one example can be sufficient
- `o`, where to write the class descriptions in json, used by `iso_pred`
- `m`, any model from HuggingFace, for example [`mistralai/Mistral-7B-Instruct-v0.2`](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) or [`meta-llama/Meta-Llama-3-8B-Instruct`](meta-llama/Meta-Llama-3-8B-Instruct)

#### Class prediction

`iso_pred -i queries.txt -c classes.json -m "mistralai/Mistral-7B-Instruct-v0.2" -o predictions.txt`

Where:
- `i`, file with queries (e.g. "human blood")
- `c`, class descriptions, generated by `iso_desc` or manullay ([example](examples/bacdive_classes.json))
- `o`, output file to write predictions to 
- `m`, any model from HuggingFace, for example [`mistralai/Mistral-7B-Instruct-v0.2`](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) or [`meta-llama/Meta-Llama-3-8B-Instruct`](meta-llama/Meta-Llama-3-8B-Instruct)

### üêí Host name 
TODO

### üåè Countries
TODO


## ü•® How it works 

### üß™ Isolation source

We use [vLLm](https://github.com/vllm-project/vllm) to do batch text-generation which is much faster compared to chat completion using for example huggingface ([benchmark](https://blog.vllm.ai/2023/06/20/vllm.html)). Since the majorify of users won't be able to use `160GB+` models (e.g. 70b+ models) we also install [AQLM](https://arxiv.org/pdf/2401.06118) that provides heavily compressed yet performant models such as [LLama-3-70b](ISTA-DASLab/Meta-Llama-3-70B-AQLM-2Bit-1x16) (160GB > 22GB) and [Command-R+](ISTA-DASLab/c4ai-command-r-plus-AQLM-2Bit-1x16) (190GB > 32GB). The latter model being the first open source model to beat the older GPT-4 on the leaderboards ([REF](https://chat.lmsys.org/?leaderboard)).The provided pipelines are relatively simple and mainly involve pre and post processing:
- `iso_desc` > get class name > create prompts > ask LLM 
- `iso_pred` > get queries > get classess > create prompts > ask LLM
*The prompts were trial and error and minor changes to a prompt can affect the output*. Moreover, re-prompting the same question might give different results. The exact prompts can be found [here](https://github.com/rickbeeloo/Interpretzel/blob/master/interpretzel/isolation_source/class_gen.py#L44) and [here](https://github.com/rickbeeloo/Interpretzel/blob/master/interpretzel/isolation_source/class_pred.py#L42). 

### üêí Host name
TODO 

### üåè Countries 
TODO


## ü•®Supported models
Basically any model that is present on [HuggingFace](https://huggingface.co/meta-llama/Meta-Llama-3-70B) as we pull the model and tokenizers from here. Since we pass instructions ideally these models are "Instruct" or "Chat" models. Base models might work, sometimes, but are probably not ideal for this task. 

We also **added support** for [AQLM](https://arxiv.org/pdf/2401.06118). This includes a wide range of super compressed models, see the full list on [their Github](https://github.com/vahe1994/AQLM?tab=readme-ov-file#models)




